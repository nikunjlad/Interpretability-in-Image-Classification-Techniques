

Understanding Interpretability
==============================

.. toctree::
   :maxdepth: 2

Everyone wants to do Machine Learning, Data Science, Deep Learning. I know,
saying out loud like - "I am a Deep Learning Engineer", itself gives the best of us goosebumps. While, I too was in the
same situation until a few years ago, I won't complain about people being in the same boat now. However, Machine Learning is
not Computer Vision. A lot of people confuse these terms. AI, ML, DL, etc. are commonly used acronyms. Its however
important to know the difference between them. Artificial intelligence is a big domain and a field comprising of many
sub-fields. In fact more than a field, it is an idea to be more subtle. Anything which can help mimic human brain in
terms of behaviour and intelligence is an artificially intelligent system. Intelligence which is artificial in nature and analogous to
humans. Machine Learning is a sub domain of Artificial Intelligence which is basically using a set of algorithms to
solve complex real world problems. Statistical Learning is a subset of Machine learning which deals with statistical
analysis and uses traditional algorithms for its computations. These algorithms include Support Vector Machines (SVM),
Linear Regression, Logistic Regression, Naive-Bayes, etc.

An important question to ask when pursuing research is **Why?** followed by **How?**. Why does this algorithm work?
How different is it from other algorithms? What is Image Segmentation in contrast to Object Detection and what goes inside
the black box models? More specifically, there was one question always running in the back of my mind - "How the heck
does this work and why?". To give you more clarity as to why this question keeps lingering in my mind, let me recall
few scenarios why you too should be having this question in your mind.

Say you are travelling in a subway and all of a sudden you find the police narrowing down on the person sitting
next to you. You wonder what just happenend and why was that person arrested. Turns out the face recognition software in
the camera mounted in the subway recognized the person next to you as a wanted criminal and notified the police with his
real time location. While everyone is happy and feeling safe that a criminal is arrested, how often do you ask a question -
"How the camera figure out the person was criminal?".

Let me give you another example. Say, you are fond of plants and you are a tree lover. You have a garden of your own and
since you are new to growing plants and you are not aware of plant species you happen to download an android application
to identify plant species. This application gives you information of plant characteristics, the weather best suitable for
its growth, the frequency at which you have to give it fertilizers and pesticides or water to keep it healthy. You point your
phone camera to a plant you just bought and the app gives a notification to water it twice everyday and to keep it in
sunlight. Moreover, it also suggests you to cut out the weed growing around the other plants of yours when you capture
the dying leaves on those plants which are slowing turning yellowish. Wouldn't you be surprised by how wizardly this
app is to let you insights of your plants and potentially save you your money. "How does it work internally and why so?".

What is Interpretability?
-------------------------

Challenges faced by current deep neural networks
------------------------------------------------

How Interpretability helps demystify black box models?
------------------------------------------------------


Understanding Explainability and its relation with Interpretability
-------------------------------------------------------------------
