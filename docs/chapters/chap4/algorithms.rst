
4. Interpretability in Computer Vision
======================================

Interpretability in Computer Vision requires algorithms which can help us understand the nuts and bolts of how deep
neural nets work. More specifically, we would like to visualize what the network is seeing and how does it see. We discuss
2 algorithms which we explored as part of our effort to interpret image classification algorithms

.. admonition:: Note

   In this work, interpretability in Image Classification is done by comparing heatmaps generated by different layers
   in the classification networks. In short, given an image, we use the below mentioned algorithms
   for a classification task and then compare the observed heatmaps as it traverses through the intermediate layers. More
   specifically we compare the Grad-CAM and Grad-CAM++ visualizations and see how they can help us interpret images when
   single instances are there and multiple instances are present. This will give us an idea as to what parts are looked
   at the most for deciding the class given an image by the particular algorithm in question.

.. toctree::
   :maxdepth: 2

   gradcam/gradcam
   gradcampp/gradcampp



